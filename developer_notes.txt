1. file_rename.sh - a shell script to rename the files in a numerical order.

2. train_test_split.py was made to split the images in 80:20 ratio. the 
'train' and 'test' directory was created as a result of this right under the
root directory from the original 'tomato_disease_dataset' which was deleted after the 
execution of this script as it was no longer needed.

3. Inception-resnet-v2 model gave 87% validation accuracy after training for 10 epochs.
(took around 7 hours in colab). Batch Size was 64

4. trained it again with image augmentations for 7 epochs, got 85% accuracy. Model 
saved as my_tomato_model.h5 (size 214 mb!). Batch Size was 64

5. Trained with mobilenet v2, got 88% accuracy for 64 batch size. size was only
7 MB!

6. Need to check Alexnet or VGGnet for batch size 64 and 32.
 EDIT: alexnet is possible only on matplotlib.

7. Trained resnet-v2-152, got 88% validation accuracy with 32 batch size. size 
of the trained model is 222mb. still preferring imagenet model.

8. Trained mobilenet with 0.0001 lr, got a val acc of 82% at 10 epochs.

10. used git lfs to push large files like inception-resnet trained models.
  for that, I had to first install git-lfs using apt. Then use git install lfs
  in the working directory.
  I then had to use a certain command: git lfs migrate import --include="my_tomato_model_inecption-resnet.h5"
  in order to push as normally pushing gave certain errors.
  
11. Trained vgg16 model for 10 epochs, got 86%val acc and only 81% acc. Gonna
train it for 20 epochs and see if any difference.
<<<<<<< HEAD

12. In order to upload the model to firebase, we have to first convert h5 model
to .pb graph and then to tflite file - for this we used h5_to_pb.py script which I got from an answer
in stackoverflow to convert h5 to .pb

13. Note: loading the trained model using keras.model.load_model yields some 
error, so I used the cutom_objects parameter while using the same function.
NOTE: RUNNING THAT SCRIPT REQUIRED INTERNET.

14. UPDATE: So you can directly convert keras models to tflite file using a small script. We also 
used a small optimization function in it to reduce the size of the model even more. The script used 
is h5_to_tflite_converter.py and the tflite file generated is "my_trained_mobilenet.tflite".
Basically you can skip 12 and 13 AND ALSO IGNORE "h5_to_pb.py" SCRIPT AND "saved_model.pb" FILE.


15. NOW I HAVE TRIED TO ADD MORE LAYERS ON TOP OF THE MOBILENET MODEL, IT SEEMS TO BE TOUCHING A 90%
VALIDATION ACCURACY WITH ONLY A SMALL INCREASE IN THE MODEL MEMORY SIZE.

ACHIEVED 92% VALIDATION ACCURACY and close to 93% accuracy WITH THIS MODEL- IT HAD AN EXTRA 512, 256 AND 64 UNITS LAYERS AFTER
THE MOBILENET LAYERS. MODEL IS SAVED AS "my_tomato_model_mobilenet(more_dense).h5"

The size of this model is only 13.5 MB.

14. I ran the h5_to_tflite.py script for the denser model of mobilenet. Saved it as my_trained_mobilenet.tflite

For some reason, I 'm getting error while running h5_to_pb.py file with this new model. So the saved_model.pb
that you see is actually from the original mobilenet trained model.



################# TRAINING IN A LOCAL GPU ###########################
1. When you run the model, you will see that tensorflow  usually allocates
 full available memory of GPU to the model which leads to the model crashing (
even though the full allocated memory is not used ). So to avoid this, we
use the lines:

physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

This limits the use of GPU memory initially itself and your training doesnt 
return any error of CUdnn failed etc etc.
